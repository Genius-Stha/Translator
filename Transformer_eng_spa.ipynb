{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "512ss4q_n1DN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVK23QUDn6fg",
        "outputId": "7522cef7-f4a2-42f9-8fc7-cc8568c48196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#import files\n",
        "#read the txt file\n",
        "with open('spa.txt') as f:\n",
        "  text=f.read()\n",
        "print(text[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tm9PX2qnteEr"
      },
      "outputs": [],
      "source": [
        "data=text.split('\\n')\n",
        "data=[i.split('\\t') for i in data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0EKpHeE8t658",
        "outputId": "0357d96f-5c42-4a93-edf3-3328084d2621"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 59482,\n  \"fields\": [\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 54913,\n        \"samples\": [\n          \"She asked me to pass her the salt.\",\n          \"It is difficult to speak three languages.\",\n          \"An accident may happen at any time.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spanish\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 57882,\n        \"samples\": [\n          \"El \\u00e1rbol se cay\\u00f3.\",\n          \"La puerta se abri\\u00f3.\",\n          \"Paren de perder el tiempo y vuelvan al trabajo.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-220f3a97-51a5-4c5f-8091-c13e730f9d28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34375</th>\n",
              "      <td>What did the pilot say?</td>\n",
              "      <td>¿Qué dijo el piloto?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90774</th>\n",
              "      <td>I will ask him about it tomorrow, then.</td>\n",
              "      <td>Entonces mañana le pregunto.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118614</th>\n",
              "      <td>Even though Tom was brought up in the country,...</td>\n",
              "      <td>A pesar de que Tom fue criado en el campo, le ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81704</th>\n",
              "      <td>Who's responsible for this problem?</td>\n",
              "      <td>¿Quién es responsable de este problema?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70137</th>\n",
              "      <td>Sing the song once more, please.</td>\n",
              "      <td>Por favor, canta la canción otra vez.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-220f3a97-51a5-4c5f-8091-c13e730f9d28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-220f3a97-51a5-4c5f-8091-c13e730f9d28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-220f3a97-51a5-4c5f-8091-c13e730f9d28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9b3e03ed-b6db-4b29-83f6-75446b0338ed\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b3e03ed-b6db-4b29-83f6-75446b0338ed')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9b3e03ed-b6db-4b29-83f6-75446b0338ed button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                  English  \\\n",
              "34375                             What did the pilot say?   \n",
              "90774             I will ask him about it tomorrow, then.   \n",
              "118614  Even though Tom was brought up in the country,...   \n",
              "81704                 Who's responsible for this problem?   \n",
              "70137                    Sing the song once more, please.   \n",
              "\n",
              "                                                  Spanish  \n",
              "34375                                ¿Qué dijo el piloto?  \n",
              "90774                        Entonces mañana le pregunto.  \n",
              "118614  A pesar de que Tom fue criado en el campo, le ...  \n",
              "81704             ¿Quién es responsable de este problema?  \n",
              "70137               Por favor, canta la canción otra vez.  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.DataFrame(data,columns=['English','Spanish'])\n",
        "\n",
        "length=len(data)\n",
        "df=df.sample(length//2)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clJjuW6gutTr",
        "outputId": "89f0add7-44ba-4d3b-da90-5fd983c9cacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 59482 entries, 34375 to 25653\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   English  59482 non-null  object\n",
            " 1   Spanish  59482 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jFx3z7KbzTxP"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RfiuBGt56ji1"
      },
      "outputs": [],
      "source": [
        "df['English']=df['English'].astype(str)\n",
        "df['Spanish']=df['Spanish'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMsHR9mA73Nl",
        "outputId": "eb0cd800-1896-45ab-fe0f-54f437673510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59482 entries, 0 to 59481\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   English  59482 non-null  object\n",
            " 1   Spanish  59482 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 929.5+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2kpFC_1C6dcw"
      },
      "outputs": [],
      "source": [
        "df['English'] = df['English'].fillna('').apply(lambda x: x.lower())\n",
        "df['Spanish'] = df['Spanish'].fillna('').apply(lambda x: x.lower())\n",
        "# df['English'] = df['English'].str.replace('.', '')\n",
        "# df['Spanish'] = df['Spanish'].str.replace('.', '')\n",
        "df['English'] = df['English'].str.replace(r'\\.', '', regex=True)\n",
        "df['Spanish'] = df['Spanish'].str.replace(r'\\.', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc6gq11tOJD"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL-6fEyhun-9",
        "outputId": "895307a7-378b-42a9-eef4-6df982193923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "spacy_es = spacy.load(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tmBEjRhTt5rs"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "spacy_es = spacy.load(\"es_core_news_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2e9rx9szt-j8"
      },
      "outputs": [],
      "source": [
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_es(text):\n",
        "    return [tok.text.lower() for tok in spacy_es.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0fFTBCMq9LQD"
      },
      "outputs": [],
      "source": [
        "df['English']=df['English'].apply(tokenize_en)\n",
        "df['Spanish']=df['Spanish'].apply(tokenize_es)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3r4otOqWuGVF",
        "outputId": "71012550-6567-4c21-b8c8-247b848aba0c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spanish\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2d218529-4b80-4894-8536-2d9226663663\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>57592</th>\n",
              "      <td>[i, saw, tom, die]</td>\n",
              "      <td>[vi, a, tom, morir]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30849</th>\n",
              "      <td>[if, you, stand, on, this, stool, ,, you, can,...</td>\n",
              "      <td>[si, te, levantas, sobre, este, taburete, ,, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9728</th>\n",
              "      <td>[no, matter, what, you, do, ,, you, must, foll...</td>\n",
              "      <td>[no, importa, como, lo, hagas, ,, pero, debes,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9919</th>\n",
              "      <td>[osaka, is, japan, 's, second, biggest, city]</td>\n",
              "      <td>[osaka, es, la, segunda, ciudad, más, grande, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12621</th>\n",
              "      <td>[i, did, n't, mean, to, frighten, you]</td>\n",
              "      <td>[no, pretendía, asustarte]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d218529-4b80-4894-8536-2d9226663663')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d218529-4b80-4894-8536-2d9226663663 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d218529-4b80-4894-8536-2d9226663663');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4a9c49f1-18bf-4651-9ad4-f3365df0e47d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a9c49f1-18bf-4651-9ad4-f3365df0e47d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4a9c49f1-18bf-4651-9ad4-f3365df0e47d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 English  \\\n",
              "57592                                 [i, saw, tom, die]   \n",
              "30849  [if, you, stand, on, this, stool, ,, you, can,...   \n",
              "9728   [no, matter, what, you, do, ,, you, must, foll...   \n",
              "9919       [osaka, is, japan, 's, second, biggest, city]   \n",
              "12621             [i, did, n't, mean, to, frighten, you]   \n",
              "\n",
              "                                                 Spanish  \n",
              "57592                                [vi, a, tom, morir]  \n",
              "30849  [si, te, levantas, sobre, este, taburete, ,, p...  \n",
              "9728   [no, importa, como, lo, hagas, ,, pero, debes,...  \n",
              "9919   [osaka, es, la, segunda, ciudad, más, grande, ...  \n",
              "12621                         [no, pretendía, asustarte]  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tnueqmTSvAuE"
      },
      "outputs": [],
      "source": [
        "#apply <start of the sentence> and <end of the sentence> in spanish which is output\n",
        "df['Spanish'] = df['Spanish'].apply(lambda x: ['<sos>'] + x + ['<eos>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "B01xEvxO5kqd",
        "outputId": "801cc9bd-4a31-4db7-f2ad-415f1ac4ef99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spanish\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c887e38d-7675-492d-9034-caeef6422abc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56077</th>\n",
              "      <td>[we, saw, the, sun, sink, below, the, horizon]</td>\n",
              "      <td>[&lt;sos&gt;, vimos, al, sol, hundirse, bajo, el, ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35410</th>\n",
              "      <td>[i, 'm, going, to, tell, you, about, a, strang...</td>\n",
              "      <td>[&lt;sos&gt;, les, voy, a, contar, un, caso, curioso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22444</th>\n",
              "      <td>[the, policeman, saved, the, child, from, drow...</td>\n",
              "      <td>[&lt;sos&gt;, el, policía, evitó, que, el, niño, se,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18231</th>\n",
              "      <td>[i, will, tell, her, what, to, say, at, the, m...</td>\n",
              "      <td>[&lt;sos&gt;, le, diré, qué, tiene, que, decir, en, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40844</th>\n",
              "      <td>[this, tie, and, that, jacket, go, well, toget...</td>\n",
              "      <td>[&lt;sos&gt;, esta, corbata, conjunta, con, aquella,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c887e38d-7675-492d-9034-caeef6422abc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c887e38d-7675-492d-9034-caeef6422abc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c887e38d-7675-492d-9034-caeef6422abc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ab8e1b7e-8ab4-4f26-9a36-1ba75b7d6c1b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab8e1b7e-8ab4-4f26-9a36-1ba75b7d6c1b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ab8e1b7e-8ab4-4f26-9a36-1ba75b7d6c1b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 English  \\\n",
              "56077     [we, saw, the, sun, sink, below, the, horizon]   \n",
              "35410  [i, 'm, going, to, tell, you, about, a, strang...   \n",
              "22444  [the, policeman, saved, the, child, from, drow...   \n",
              "18231  [i, will, tell, her, what, to, say, at, the, m...   \n",
              "40844  [this, tie, and, that, jacket, go, well, toget...   \n",
              "\n",
              "                                                 Spanish  \n",
              "56077  [<sos>, vimos, al, sol, hundirse, bajo, el, ho...  \n",
              "35410  [<sos>, les, voy, a, contar, un, caso, curioso...  \n",
              "22444  [<sos>, el, policía, evitó, que, el, niño, se,...  \n",
              "18231  [<sos>, le, diré, qué, tiene, que, decir, en, ...  \n",
              "40844  [<sos>, esta, corbata, conjunta, con, aquella,...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt2n52Zcz1V2",
        "outputId": "f71543b5-3ce3-4251-8098-82c9565a2027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40\n",
            "37\n"
          ]
        }
      ],
      "source": [
        "spalen=0\n",
        "for x in df['Spanish']:\n",
        "  if len(x)>spalen:\n",
        "    spalen=len(x)\n",
        "print(spalen)\n",
        "\n",
        "englen=0\n",
        "for x in df['English']:\n",
        "  if len(x)>englen:\n",
        "    englen=len(x)\n",
        "print(englen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XrpcNnoZwry5"
      },
      "outputs": [],
      "source": [
        "df['English'] = df['English'].apply(lambda x: x + ['<pad>'] * (englen - len(x)))\n",
        "df['Spanish'] = df['Spanish'].apply(lambda x: x + ['<pad>'] * (spalen - len(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXm13IrY2uJq",
        "outputId": "ac551a42-eb20-4cf7-f46d-042fba2cf7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10392\n",
            "19210\n"
          ]
        }
      ],
      "source": [
        "\n",
        "engword = df['English'].explode().unique()\n",
        "spaword = df['Spanish'].explode().unique()\n",
        "print(len(engword))\n",
        "print(len(spaword))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8bLpWSsz40Vl"
      },
      "outputs": [],
      "source": [
        "eng_tok_stoi = dict([(char, i) for i,char in enumerate(engword)])\n",
        "spa_tok_stoi = dict([(char, i) for i,char in enumerate(spaword)])\n",
        "\n",
        "eng_tok_itos = dict([(i,char)for i ,char in enumerate(engword)])\n",
        "spa_tok_itos = dict([(i,char)for i ,char in enumerate(spaword)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MYASJKlu5a71"
      },
      "outputs": [],
      "source": [
        "df['English']=df['English'].apply(lambda x: [eng_tok_stoi[word] for word in x])\n",
        "df['Spanish']=df['Spanish'].apply(lambda x: [spa_tok_stoi[word] for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTd98mchAusc",
        "outputId": "a5c372cd-474a-4450-fadf-87451cedcae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English vocab: 10392\n",
            "Spanish vocab: 19210\n"
          ]
        }
      ],
      "source": [
        "print('English vocab:',len(eng_tok_itos))\n",
        "print('Spanish vocab:',len(spa_tok_itos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-0QAYm556pv",
        "outputId": "5a3aee99-b56e-40bf-8118-1e2bc0f47e2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47585"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_len = int(len(df)*0.8)\n",
        "train_len\n",
        "#47585 rows of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kU6JnuMn9dgS"
      },
      "outputs": [],
      "source": [
        "X_train = df['English'].iloc[:int(train_len)]\n",
        "y_train = df['Spanish'].iloc[:int(train_len)]\n",
        "X_test = df['English'].iloc[int(train_len):]\n",
        "y_test = df['Spanish'].iloc[int(train_len):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zNRoerca6Lw_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, english_data, spanish_data):\n",
        "        self.english_data = english_data\n",
        "        self.spanish_data = spanish_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_seq = torch.tensor(self.english_data.iloc[idx], dtype=torch.long)\n",
        "        spanish_seq = torch.tensor(self.spanish_data.iloc[idx], dtype=torch.long)\n",
        "        return english_seq, spanish_seq\n",
        "\n",
        "# Assuming you have already created your df, englen, spalen, eng_tok_index, spa_tok_index\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = TranslationDataset(X_train, y_train)\n",
        "test_dataset = TranslationDataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoader instances\n",
        "batch_size = 64 # You can adjust this\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fClUkgFKcHT1",
        "outputId": "fbca82dc-8d4f-4e39-c4e5-2ea99cfc7f3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "744"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count=0\n",
        "for english_batch, spanish_batch in train_dataloader:\n",
        "  count+=1\n",
        "count\n",
        "#744*64 ~= 47585\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaQdO9xOdXeV",
        "outputId": "4a6ebd70-f8e4-42e4-a4ad-3ebc3f338f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Batch Shape: torch.Size([64, 37])\n",
            "Spanish Batch Shape: torch.Size([64, 40])\n"
          ]
        }
      ],
      "source": [
        "for english_batch, spanish_batch in train_dataloader:\n",
        "  print(\"English Batch Shape:\", english_batch.shape)\n",
        "  print(\"Spanish Batch Shape:\", spanish_batch.shape)\n",
        "  break\n",
        "#the train_dataloader is of shape (744,64,37) (744,64,40) for english and spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nIyS58xoDbqa"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size, dropout, maxlen=5000):\n",
        "        super().__init__()\n",
        "        #torch.arange(0, emb_size, 2) is i=[0, dmodel/2]\n",
        "        #- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size\n",
        "        #=>2i/emb_size(d_model) * -log of 10000 #x*loga =loga^x\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PI-iJD60oTRZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import pandas as pd # For dummy data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aZ_B7vnloX-L"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize_en_placeholder(text: str):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "def tokenize_es_placeholder(text):\n",
        "    return [tok.text.lower() for tok in spacy_es.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b24YJoupoYpD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Transformer Model Components ---\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional encoding module.\n",
        "    Adds positional information to the input embeddings.\n",
        "    Expected input shape: [batch_size, seq_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create a positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [max_len, 1]\n",
        "        # Term for calculating frequencies: e^( (0, 2, ..., d_model-2) * (-log(10000)/d_model) )\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension: [1, max_len, d_model]\n",
        "        # This allows broadcasting across the batch\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # Register 'pe' as a buffer, so it's part of the model's state\n",
        "        # but not updated by the optimizer.\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [batch_size, seq_len, d_model]\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, seq_len, d_model] with positional encodings added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input tensor x.\n",
        "        # self.pe is [1, max_len, d_model]. We slice it to match x's seq_len.\n",
        "        # self.pe[:, :x.size(1), :] gives [1, seq_len, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MZuxPbVWoiI6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention mechanism.\n",
        "    Allows the model to jointly attend to information from different\n",
        "    representation subspaces at different positions.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # d_model: Total dimension of the model (e.g., 512)\n",
        "        # n_heads: Number of attention heads (e.g., 8)\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # Dimension of each head's key/query/value (e.g., 512/8 = 64)\n",
        "\n",
        "        # Linear layers for Query, Key, Value, and Output\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
        "                                     mask: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        Computes scaled dot-product attention.\n",
        "        Args:\n",
        "            Q: Query tensor, shape [batch_size, n_heads, seq_len_q, d_k]\n",
        "            K: Key tensor, shape [batch_size, n_heads, seq_len_k, d_k]\n",
        "            V: Value tensor, shape [batch_size, n_heads, seq_len_v, d_k]\n",
        "            =>sequece_length of q k can be different in decoder\n",
        "            q is created from the target that is querying and\n",
        "            k and v is created from the source that is being searched.\n",
        "            mask: Optional mask tensor, shape broadcastable to [batch_size, n_heads, seq_len_q, seq_len_k]\n",
        "                  Masked positions should be True or 1.\n",
        "        Returns:\n",
        "            context: Output tensor after attention, shape [batch_size, n_heads, seq_len_q, d_k]\n",
        "            attention_weights: Attention weights, shape [batch_size, n_heads, seq_len_q, seq_len_k]\n",
        "        \"\"\"\n",
        "        # MatMul(Q, K.transpose) / sqrt(d_k)\n",
        "        # K.transpose(-2, -1) swaps the last two dimensions (seq_len_k and d_k)\n",
        "        # Note : Matrix multiplication is used in last two layer only\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [B, H, Lq, Lk]\n",
        "\n",
        "        # Apply mask if provided (e.g., for padding or look-ahead)\n",
        "        if mask is not None:\n",
        "            # Masked positions (where mask is True, or 0 if mask indicates valid positions)\n",
        "            # Assuming mask indicates valid positions with 1/True, invalid with 0/False.\n",
        "            scores = scores.masked_fill(mask == 0, -1e9) # Fill where mask is False\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [B, H, Lq, Lk]\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # MatMul(attention_weights, V)\n",
        "        context = torch.matmul(attention_weights, V) # [B, H, Lq, d_k]\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "                mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for MultiHeadAttention.\n",
        "        Args:\n",
        "            query: Query tensor, shape [batch_size, seq_len_q, d_model]\n",
        "            key: Key tensor, shape [batch_size, seq_len_k, d_model]\n",
        "            value: Value tensor, shape [batch_size, seq_len_v, d_model] (seq_len_k == seq_len_v)\n",
        "            mask: Optional mask, shape broadcastable to [batch_size, 1, seq_len_q, seq_len_k]\n",
        "        Returns:\n",
        "            output: Output tensor, shape [batch_size, seq_len_q, d_model]\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 1. Linear transformations and reshape into multiple heads\n",
        "        # query, key, value shapes: [batch_size, seq_len, d_model]\n",
        "        # After W_q, W_k, W_v: still [batch_size, seq_len, d_model]\n",
        "        # .view: [batch_size, seq_len, n_heads, d_k]\n",
        "        # .transpose(1, 2): [batch_size, n_heads, seq_len, d_k]\n",
        "        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 2. Apply scaled dot-product attention\n",
        "        # The mask for scaled_dot_product_attention should be [B, H, Lq, Lk] or broadcastable.\n",
        "        # If input mask is [B, 1, Lq, Lk] or [B, 1, 1, Lk], it will broadcast.\n",
        "        attention_context, _ = self.scaled_dot_product_attention(Q, K, V, mask) # attention_context: [B, H, Lq, d_k]\n",
        "\n",
        "        # 3. Concatenate heads and apply final linear layer\n",
        "        # .transpose(1, 2): [batch_size, seq_len_q, n_heads, d_k]\n",
        "        # .contiguous().view: [batch_size, seq_len_q, d_model (n_heads * d_k)]\n",
        "        attention_context = attention_context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.W_o(attention_context) # [batch_size, seq_len_q, d_model]\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pqeAJCbAonKW"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise feed-forward network.\n",
        "    Applied to each position separately and identically.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # d_model: Input and output dimension (e.g., 512)\n",
        "        # d_ff: Inner dimension of the FFN (e.g., 2048)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "        # Here, using ReLU as activation\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "817RYCjoopQy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single encoder layer, consisting of self-attention and a feed-forward network.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # Layer normalization for self-attention\n",
        "        self.norm2 = nn.LayerNorm(d_model) # Layer normalization for feed-forward\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Input to the encoder layer, shape [batch_size, src_len, d_model]\n",
        "            src_mask: Mask for source sequence (e.g., padding mask),\n",
        "                      shape broadcastable to [batch_size, n_heads, src_len, src_len]\n",
        "        Returns:\n",
        "            Output tensor, shape [batch_size, src_len, d_model]\n",
        "        \"\"\"\n",
        "        # 1. Self-attention sublayer\n",
        "        # Q, K, V are all 'src'\n",
        "        attn_output = self.self_attention(src, src, src, src_mask)\n",
        "        # Residual connection and layer normalization\n",
        "        src = self.norm1(src + self.dropout(attn_output))\n",
        "\n",
        "        # 2. Feed-forward sublayer\n",
        "        ff_output = self.feed_forward(src)\n",
        "        # Residual connection and layer normalization\n",
        "        src = self.norm2(src + self.dropout(ff_output))\n",
        "\n",
        "        return src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DcgMbFHiorbq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single decoder layer, consisting of self-attention, cross-attention (encoder-decoder attention),\n",
        "    and a feed-forward network.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout) # Masked self-attention\n",
        "        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout) # Encoder-decoder attention\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # For self-attention\n",
        "        self.norm2 = nn.LayerNorm(d_model) # For cross-attention\n",
        "        self.norm3 = nn.LayerNorm(d_model) # For feed-forward\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt: torch.Tensor, encoder_output: torch.Tensor,\n",
        "                src_mask: torch.Tensor = None, tgt_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: Input to the decoder layer (target sequence embeddings), shape [batch_size, tgt_len, d_model]\n",
        "            encoder_output: Output from the encoder, shape [batch_size, src_len, d_model]\n",
        "            src_mask: Mask for source sequence (padding mask from encoder),\n",
        "                      shape broadcastable to [batch_size, n_heads, tgt_len (query), src_len (key)]\n",
        "            tgt_mask: Mask for target sequence (e.g., combined look-ahead and padding mask),\n",
        "                      shape broadcastable to [batch_size, n_heads, tgt_len, tgt_len]\n",
        "        Returns:\n",
        "            Output tensor, shape [batch_size, tgt_len, d_model]\n",
        "        \"\"\"\n",
        "        # 1. Masked Self-Attention sublayer (on target sequence)\n",
        "        # Q, K, V are all 'tgt'\n",
        "        self_attn_output = self.self_attention(tgt, tgt, tgt, tgt_mask)\n",
        "        # Residual connection and layer normalization\n",
        "        tgt = self.norm1(tgt + self.dropout(self_attn_output))\n",
        "\n",
        "        # 2. Cross-Attention sublayer (attending to encoder output)\n",
        "        # Query is 'tgt' (decoder state), Key and Value are 'encoder_output'\n",
        "        cross_attn_output = self.cross_attention(tgt, encoder_output, encoder_output, src_mask)\n",
        "        # Residual connection and layer normalization\n",
        "        tgt = self.norm2(tgt + self.dropout(cross_attn_output))\n",
        "\n",
        "        # 3. Feed-Forward sublayer\n",
        "        ff_output = self.feed_forward(tgt)\n",
        "        # Residual connection and layer normalization\n",
        "        tgt = self.norm3(tgt + self.dropout(ff_output))\n",
        "\n",
        "        return tgt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8SHkkwHQot16"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer encoder, stacking multiple EncoderLayers.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size: int, d_model: int, n_heads: int,\n",
        "                 n_layers: int, d_ff: int, max_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout for the sum of embeddings and pos encoding\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Source sequence tensor (token indices), shape [batch_size, src_len]\n",
        "            src_mask: Mask for source sequence.\n",
        "        Returns:\n",
        "            Output tensor from the encoder, shape [batch_size, src_len, d_model]\n",
        "        \"\"\"\n",
        "        # src shape: [batch_size, src_len]\n",
        "        # Embedding: [batch_size, src_len, d_model]\n",
        "        # Scale embedding by sqrt(d_model) as per \"Attention is All You Need\"\n",
        "        embedded_src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        # Add positional encoding\n",
        "        pos_encoded_src = self.pos_encoding(embedded_src)\n",
        "        # Dropout is applied by PositionalEncoding, or can be applied here too if not in PE.\n",
        "\n",
        "        # Pass through each encoder layer\n",
        "        encoder_output = pos_encoded_src\n",
        "        for layer in self.layers:\n",
        "            encoder_output = layer(encoder_output, src_mask)\n",
        "\n",
        "        return encoder_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NRPTN6efoxE-"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer decoder, stacking multiple DecoderLayers.\n",
        "    \"\"\"\n",
        "    def __init__(self, tgt_vocab_size: int, d_model: int, n_heads: int,\n",
        "                 n_layers: int, d_ff: int, max_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout for the sum of embeddings and pos encoding\n",
        "        self.output_projection = nn.Linear(d_model, tgt_vocab_size) # To get final logits\n",
        "\n",
        "    def forward(self, tgt: torch.Tensor, encoder_output: torch.Tensor,\n",
        "                src_mask: torch.Tensor = None, tgt_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: Target sequence tensor (token indices), shape [batch_size, tgt_len]\n",
        "            encoder_output: Output from the encoder, shape [batch_size, src_len, d_model]\n",
        "            src_mask: Mask for source sequence (for cross-attention).\n",
        "            tgt_mask: Mask for target sequence (for self-attention).\n",
        "        Returns:\n",
        "            Output logits tensor, shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "        \"\"\"\n",
        "        # tgt shape: [batch_size, tgt_len]\n",
        "        # Embedding: [batch_size, tgt_len, d_model]\n",
        "        embedded_tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        # Add positional encoding\n",
        "        pos_encoded_tgt = self.pos_encoding(embedded_tgt)\n",
        "        # Dropout is applied by PositionalEncoding.\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        decoder_output = pos_encoded_tgt\n",
        "        for layer in self.layers:\n",
        "            decoder_output = layer(decoder_output, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Final linear layer to project to vocabulary size\n",
        "        output_logits = self.output_projection(decoder_output)\n",
        "\n",
        "        return output_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "l9JIqRFCo1PU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer model combining Encoder and Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 512,\n",
        "                 n_heads: int = 8, n_encoder_layers: int = 6, n_decoder_layers: int = 6,\n",
        "                 d_ff: int = 2048, max_len: int = 100, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(\n",
        "            src_vocab_size, d_model, n_heads, n_encoder_layers, d_ff, max_len, dropout\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(\n",
        "            tgt_vocab_size, d_model, n_heads, n_decoder_layers, d_ff, max_len, dropout\n",
        "        )\n",
        "        # It's good practice to initialize weights, e.g., Xavier uniform.\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:         #only the weight tensor not bias\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_padding_mask(self, seq: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Creates a mask to hide padding tokens.\n",
        "        Args:\n",
        "            seq: Input sequence tensor, shape [batch_size, seq_len]\n",
        "            pad_idx: Index of the padding token.\n",
        "        Returns:\n",
        "            mask: Boolean tensor, shape [batch_size, 1, 1, seq_len].\n",
        "                  True for non-padding, False for padding.\n",
        "                  Ready for broadcasting in MultiHeadAttention.\n",
        "        \"\"\"\n",
        "        # seq != pad_idx results in a boolean tensor: True where not pad, False where pad.\n",
        "        # .unsqueeze(1).unsqueeze(2) adds dimensions for broadcasting with attention scores\n",
        "        # [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
        "        return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def create_look_ahead_mask(self, size: int, device: torch.device) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Creates a look-ahead mask to prevent attending to future tokens in self-attention.\n",
        "        Args:\n",
        "            size: Sequence length of the target.\n",
        "            device: The device to create the mask on.\n",
        "        Returns:\n",
        "            mask: Boolean tensor, shape [1, 1, size, size].\n",
        "                  Lower triangle is True, upper is False.\n",
        "        \"\"\"\n",
        "        # torch.ones creates a matrix of ones\n",
        "        # torch.tril gets the lower triangular part\n",
        "        mask = torch.tril(torch.ones(size, size, device=device)).bool() # [size, size]\n",
        "        # .unsqueeze(0).unsqueeze(0) adds dimensions for broadcasting\n",
        "        return mask.unsqueeze(0).unsqueeze(0) # [1, 1, size, size]\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
        "                src_pad_idx: int, tgt_pad_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the Transformer model.\n",
        "        Args:\n",
        "            src: Source sequence tensor (token indices), shape [batch_size, src_len]\n",
        "            tgt: Target sequence tensor (token indices), shape [batch_size, tgt_len]\n",
        "            src_pad_idx: Padding index for the source vocabulary.\n",
        "            tgt_pad_idx: Padding index for the target vocabulary.\n",
        "        Returns:\n",
        "            Output logits tensor from the decoder, shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "        \"\"\"\n",
        "        # 1. Create masks\n",
        "        # Source padding mask (for encoder self-attention and decoder cross-attention)\n",
        "        src_mask = self.create_padding_mask(src, src_pad_idx)  # [B, 1, 1, L_src]\n",
        "\n",
        "        # Target padding mask (for decoder self-attention)\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt, tgt_pad_idx)  # [B, 1, 1, L_tgt]\n",
        "\n",
        "        # Target look-ahead mask (for decoder self-attention to prevent seeing future tokens)\n",
        "        look_ahead_mask = self.create_look_ahead_mask(tgt.size(1), tgt.device)  # [1, 1, L_tgt, L_tgt]\n",
        "\n",
        "        # Combine target padding mask with look-ahead mask for decoder self-attention\n",
        "        # Both should be True for a position to be attended.\n",
        "        # tgt_padding_mask broadcasted: [B, 1, 1, L_tgt] -> [B, 1, L_tgt, L_tgt] (conceptually for the &)\n",
        "        # look_ahead_mask: [1, 1, L_tgt, L_tgt]\n",
        "        # Resulting tgt_mask: [B, 1, L_tgt, L_tgt]\n",
        "        tgt_mask = tgt_padding_mask & look_ahead_mask\n",
        "\n",
        "        # 2. Encoder pass\n",
        "        encoder_output = self.encoder(src, src_mask) # encoder_output: [B, L_src, D_model]\n",
        "\n",
        "        # 3. Decoder pass\n",
        "        # src_mask for cross-attention is the same src_padding_mask.\n",
        "        # tgt_mask is the combined mask for self-attention in decoder.\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        return decoder_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "A1SsNZQ-o26m"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Dataset and DataLoader ---\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for translation tasks.\n",
        "    Assumes src_data and tgt_data are Pandas Series where each element is a list of token indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_data: pd.Series, tgt_data: pd.Series):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        # .iloc[idx] retrieves the list of token indices from the Pandas Series\n",
        "        src_tensor = torch.tensor(self.src_data.iloc[idx], dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor(self.tgt_data.iloc[idx], dtype=torch.long)\n",
        "        return src_tensor, tgt_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fnpXTNwBsZw7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Assuming 'Transformer' is a defined class and imported\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def train_epoch(model: Transformer, dataloader: DataLoader, optimizer: torch.optim.Optimizer,\n",
        "                criterion: nn.Module, device: torch.device, clip: float = 1.0,\n",
        "                src_pad_idx: int = 0, tgt_pad_idx: int = 0, current_epoch: int = 0, total_epochs: int = 0) -> float:\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device) # src/tgt: [batch_size, seq_len]\n",
        "\n",
        "        # Prepare target sequences for teacher forcing:\n",
        "        # Decoder input: <sos> ... token_N (remove <eos>)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        # Target output: token_1 ... <eos> (remove <sos>)\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        # The model's forward method handles mask creation internally if pad_idx is passed,\n",
        "        # or we can pass them explicitly. Here, we pass pad_idx values.\n",
        "        output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "        # output_logits shape: [batch_size, tgt_len-1, tgt_vocab_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        # Reshape for CrossEntropyLoss:\n",
        "        # output_logits: [(batch_size * (tgt_len-1)), tgt_vocab_size]\n",
        "        # tgt_output: [batch_size * (tgt_len-1)]\n",
        "        loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "        loss.backward() # Compute gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Gradient clipping\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0 or batch_idx == num_batches -1:\n",
        "            print(f'  Epoch [{current_epoch+1}/{total_epochs}], Batch [{batch_idx+1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def evaluate_epoch(model: Transformer, dataloader: DataLoader, criterion: nn.Module,\n",
        "                   device: torch.device, src_pad_idx: int = 0, tgt_pad_idx: int = 0) -> float:\n",
        "    \"\"\"Evaluates the model for one epoch.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "            loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / num_batches\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device) # src/tgt: [batch_size, seq_len]\n",
        "\n",
        "        # Prepare target sequences for teacher forcing:\n",
        "        # Decoder input: <sos> ... token_N (remove <eos>)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        # Target output: token_1 ... <eos> (remove <sos>)\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        # The model's forward method handles mask creation internally if pad_idx is passed,\n",
        "        # or we can pass them explicitly. Here, we pass pad_idx values.\n",
        "        output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "        # output_logits shape: [batch_size, tgt_len-1, tgt_vocab_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        # Reshape for CrossEntropyLoss:\n",
        "        # output_logits: [(batch_size * (tgt_len-1)), tgt_vocab_size]\n",
        "        # tgt_output: [batch_size * (tgt_len-1)]\n",
        "        loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "        loss.backward() # Compute gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Gradient clipping\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0 or batch_idx == num_batches -1:\n",
        "            print(f'  Epoch [{current_epoch+1}/{total_epochs}], Batch [{batch_idx+1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5Z5zb0Hqyung"
      },
      "outputs": [],
      "source": [
        "# --- Training and Evaluation Functions ---\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time # Import time for timing epochs\n",
        "\n",
        "# Assuming 'Transformer' is a defined class and imported\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def train_epoch(model: Transformer, dataloader: DataLoader, optimizer: torch.optim.Optimizer,\n",
        "                criterion: nn.Module, device: torch.device, clip: float = 1.0,\n",
        "                src_pad_idx: int = 0, tgt_pad_idx: int = 0, current_epoch: int = 0, total_epochs: int = 0) -> float:\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device) # src/tgt: [batch_size, seq_len]\n",
        "\n",
        "        # Prepare target sequences for teacher forcing:\n",
        "        # Decoder input: <sos> ... token_N (remove <eos>)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        # Target output: token_1 ... <eos> (remove <sos>)\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        # The model's forward method handles mask creation internally if pad_idx is passed,\n",
        "        # or we can pass them explicitly. Here, we pass pad_idx values.\n",
        "        output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "        # output_logits shape: [batch_size, tgt_len-1, tgt_vocab_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        # Reshape for CrossEntropyLoss:\n",
        "        # output_logits: [(batch_size * (tgt_len-1)), tgt_vocab_size]\n",
        "        # tgt_output: [batch_size * (tgt_len-1)]\n",
        "        loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "        loss.backward() # Compute gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Gradient clipping\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Added a print statement for progress feedback\n",
        "        if (batch_idx + 1) % 100 == 0 or batch_idx == num_batches -1:\n",
        "            print(f'  Epoch [{current_epoch+1}/{total_epochs}], Batch [{batch_idx+1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def evaluate_epoch(model: Transformer, dataloader: DataLoader, criterion: nn.Module,\n",
        "                   device: torch.device, src_pad_idx: int = 0, tgt_pad_idx: int = 0) -> float:\n",
        "    \"\"\"Evaluates the model for one epoch.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "            loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def train_transformer_model(model: Transformer, train_dataloader: DataLoader, test_dataloader: DataLoader,\n",
        "                           num_epochs: int, device: torch.device, learning_rate: float, warmup_steps: int,\n",
        "                           src_pad_idx: int, tgt_pad_idx: int, clip: float = 1.0, model_save_path: str = None):\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    # Ignore padding index in loss calculation\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device,\n",
        "                                 clip, src_pad_idx, tgt_pad_idx, epoch, num_epochs)\n",
        "\n",
        "        # Evaluate\n",
        "        valid_loss = evaluate_epoch(model, test_dataloader, criterion, device,\n",
        "                                    src_pad_idx, tgt_pad_idx)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.0f}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "        print(f'\\t Valid Loss: {valid_loss:.3f}')\n",
        "\n",
        "        # Save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            if model_save_path:\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "                print(f'Saved best model to {model_save_path}')\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model # Optionally return the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kAtqMfHqsbro"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_epoch(model: Transformer, dataloader: DataLoader, criterion: nn.Module,\n",
        "                   device: torch.device, src_pad_idx: int, tgt_pad_idx: int) -> float:\n",
        "    \"\"\"Evaluates the model for one epoch.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "            loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / num_batches\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device) # src/tgt: [batch_size, seq_len]\n",
        "\n",
        "        # Prepare target sequences for teacher forcing:\n",
        "        # Decoder input: <sos> ... token_N (remove <eos>)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        # Target output: token_1 ... <eos> (remove <sos>)\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        # The model's forward method handles mask creation internally if pad_idx is passed,\n",
        "        # or we can pass them explicitly. Here, we pass pad_idx values.\n",
        "        output_logits = model(src, tgt_input, src_pad_idx=src_pad_idx, tgt_pad_idx=tgt_pad_idx)\n",
        "        # output_logits shape: [batch_size, tgt_len-1, tgt_vocab_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        # Reshape for CrossEntropyLoss:\n",
        "        # output_logits: [(batch_size * (tgt_len-1)), tgt_vocab_size]\n",
        "        # tgt_output: [batch_size * (tgt_len-1)]\n",
        "        loss = criterion(output_logits.reshape(-1, output_logits.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "        loss.backward() # Compute gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Gradient clipping\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0 or batch_idx == num_batches -1:\n",
        "            print(f'  Epoch [{current_epoch+1}/{total_epochs}], Batch [{batch_idx+1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyS8hwN_2O-A"
      },
      "source": [
        "# MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKi2ePO8sBnP",
        "outputId": "c4d579a5-3e39-45fb-d5d8-a83600a3c23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Determined MAX_SEQ_LEN from data: 40\n",
            "Using device: cuda\n",
            "Transformer Model initialized with 16,468,746 trainable parameters.\n",
            "  Epoch [1/10], Batch [100/744], Loss: 5.7439\n",
            "  Epoch [1/10], Batch [200/744], Loss: 5.4637\n",
            "  Epoch [1/10], Batch [300/744], Loss: 5.2471\n",
            "  Epoch [1/10], Batch [400/744], Loss: 4.8346\n",
            "  Epoch [1/10], Batch [500/744], Loss: 4.6414\n",
            "  Epoch [1/10], Batch [600/744], Loss: 4.3983\n",
            "  Epoch [1/10], Batch [700/744], Loss: 4.3128\n",
            "  Epoch [1/10], Batch [744/744], Loss: 4.2308\n",
            "Epoch: 01 | Time: 0.0m 49s\n",
            "\tTrain Loss: 5.119\n",
            "\t Valid Loss: 4.148\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [2/10], Batch [100/744], Loss: 4.0693\n",
            "  Epoch [2/10], Batch [200/744], Loss: 3.9534\n",
            "  Epoch [2/10], Batch [300/744], Loss: 3.9136\n",
            "  Epoch [2/10], Batch [400/744], Loss: 3.3747\n",
            "  Epoch [2/10], Batch [500/744], Loss: 3.5134\n",
            "  Epoch [2/10], Batch [600/744], Loss: 3.2371\n",
            "  Epoch [2/10], Batch [700/744], Loss: 3.2565\n",
            "  Epoch [2/10], Batch [744/744], Loss: 3.4447\n",
            "Epoch: 02 | Time: 0.0m 48s\n",
            "\tTrain Loss: 3.655\n",
            "\t Valid Loss: 3.164\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [3/10], Batch [100/744], Loss: 3.0048\n",
            "  Epoch [3/10], Batch [200/744], Loss: 3.2511\n",
            "  Epoch [3/10], Batch [300/744], Loss: 2.7281\n",
            "  Epoch [3/10], Batch [400/744], Loss: 2.6481\n",
            "  Epoch [3/10], Batch [500/744], Loss: 3.0412\n",
            "  Epoch [3/10], Batch [600/744], Loss: 2.8908\n",
            "  Epoch [3/10], Batch [700/744], Loss: 2.9023\n",
            "  Epoch [3/10], Batch [744/744], Loss: 2.6104\n",
            "Epoch: 03 | Time: 0.0m 48s\n",
            "\tTrain Loss: 2.911\n",
            "\t Valid Loss: 2.704\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [4/10], Batch [100/744], Loss: 2.3964\n",
            "  Epoch [4/10], Batch [200/744], Loss: 2.5489\n",
            "  Epoch [4/10], Batch [300/744], Loss: 2.5106\n",
            "  Epoch [4/10], Batch [400/744], Loss: 2.3771\n",
            "  Epoch [4/10], Batch [500/744], Loss: 2.7101\n",
            "  Epoch [4/10], Batch [600/744], Loss: 2.4093\n",
            "  Epoch [4/10], Batch [700/744], Loss: 2.2797\n",
            "  Epoch [4/10], Batch [744/744], Loss: 2.0867\n",
            "Epoch: 04 | Time: 0.0m 49s\n",
            "\tTrain Loss: 2.481\n",
            "\t Valid Loss: 2.429\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [5/10], Batch [100/744], Loss: 2.0909\n",
            "  Epoch [5/10], Batch [200/744], Loss: 1.9147\n",
            "  Epoch [5/10], Batch [300/744], Loss: 2.2488\n",
            "  Epoch [5/10], Batch [400/744], Loss: 2.2231\n",
            "  Epoch [5/10], Batch [500/744], Loss: 2.3314\n",
            "  Epoch [5/10], Batch [600/744], Loss: 2.2088\n",
            "  Epoch [5/10], Batch [700/744], Loss: 2.3356\n",
            "  Epoch [5/10], Batch [744/744], Loss: 2.2905\n",
            "Epoch: 05 | Time: 0.0m 48s\n",
            "\tTrain Loss: 2.198\n",
            "\t Valid Loss: 2.288\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [6/10], Batch [100/744], Loss: 1.8041\n",
            "  Epoch [6/10], Batch [200/744], Loss: 1.8238\n",
            "  Epoch [6/10], Batch [300/744], Loss: 2.1697\n",
            "  Epoch [6/10], Batch [400/744], Loss: 2.0056\n",
            "  Epoch [6/10], Batch [500/744], Loss: 2.2121\n",
            "  Epoch [6/10], Batch [600/744], Loss: 1.9838\n",
            "  Epoch [6/10], Batch [700/744], Loss: 2.1622\n",
            "  Epoch [6/10], Batch [744/744], Loss: 1.8663\n",
            "Epoch: 06 | Time: 0.0m 48s\n",
            "\tTrain Loss: 2.004\n",
            "\t Valid Loss: 2.193\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [7/10], Batch [100/744], Loss: 2.0255\n",
            "  Epoch [7/10], Batch [200/744], Loss: 1.8002\n",
            "  Epoch [7/10], Batch [300/744], Loss: 2.0014\n",
            "  Epoch [7/10], Batch [400/744], Loss: 2.0344\n",
            "  Epoch [7/10], Batch [500/744], Loss: 2.0152\n",
            "  Epoch [7/10], Batch [600/744], Loss: 1.8540\n",
            "  Epoch [7/10], Batch [700/744], Loss: 1.7375\n",
            "  Epoch [7/10], Batch [744/744], Loss: 1.9096\n",
            "Epoch: 07 | Time: 0.0m 48s\n",
            "\tTrain Loss: 1.852\n",
            "\t Valid Loss: 2.132\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [8/10], Batch [100/744], Loss: 1.5341\n",
            "  Epoch [8/10], Batch [200/744], Loss: 1.9060\n",
            "  Epoch [8/10], Batch [300/744], Loss: 1.7325\n",
            "  Epoch [8/10], Batch [400/744], Loss: 1.8486\n",
            "  Epoch [8/10], Batch [500/744], Loss: 1.6816\n",
            "  Epoch [8/10], Batch [600/744], Loss: 1.7790\n",
            "  Epoch [8/10], Batch [700/744], Loss: 1.7256\n",
            "  Epoch [8/10], Batch [744/744], Loss: 1.6379\n",
            "Epoch: 08 | Time: 0.0m 48s\n",
            "\tTrain Loss: 1.734\n",
            "\t Valid Loss: 2.113\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [9/10], Batch [100/744], Loss: 1.5715\n",
            "  Epoch [9/10], Batch [200/744], Loss: 1.6459\n",
            "  Epoch [9/10], Batch [300/744], Loss: 1.4909\n",
            "  Epoch [9/10], Batch [400/744], Loss: 1.4778\n",
            "  Epoch [9/10], Batch [500/744], Loss: 1.7413\n",
            "  Epoch [9/10], Batch [600/744], Loss: 1.7693\n",
            "  Epoch [9/10], Batch [700/744], Loss: 1.8596\n",
            "  Epoch [9/10], Batch [744/744], Loss: 1.5092\n",
            "Epoch: 09 | Time: 0.0m 48s\n",
            "\tTrain Loss: 1.638\n",
            "\t Valid Loss: 2.108\n",
            "Saved best model to best_transformer_model.pt\n",
            "  Epoch [10/10], Batch [100/744], Loss: 1.5830\n",
            "  Epoch [10/10], Batch [200/744], Loss: 1.4412\n",
            "  Epoch [10/10], Batch [300/744], Loss: 1.6444\n",
            "  Epoch [10/10], Batch [400/744], Loss: 1.3317\n",
            "  Epoch [10/10], Batch [500/744], Loss: 1.6032\n",
            "  Epoch [10/10], Batch [600/744], Loss: 1.6980\n",
            "  Epoch [10/10], Batch [700/744], Loss: 1.7598\n",
            "  Epoch [10/10], Batch [744/744], Loss: 1.7179\n",
            "Epoch: 10 | Time: 0.0m 48s\n",
            "\tTrain Loss: 1.566\n",
            "\t Valid Loss: 2.067\n",
            "Saved best model to best_transformer_model.pt\n",
            "Training complete.\n",
            "\n",
            "Loaded best model from best_transformer_model.pt for translation.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    SRC_VOCAB_SIZE = len(engword) # Use the size of the English vocabulary\n",
        "    TGT_VOCAB_SIZE = len(spaword) # Use the size of the Spanish vocabulary\n",
        "\n",
        "    max_eng_len = 0\n",
        "    max_spa_len = 0\n",
        "    for eng_batch, spa_batch in train_dataloader:\n",
        "        max_eng_len = max(max_eng_len, eng_batch.size(1))\n",
        "        max_spa_len = max(max_spa_len, spa_batch.size(1))\n",
        "\n",
        "    # MAX_SEQ_LEN should be at least the maximum length found in the data.\n",
        "    # The decoder input tgt_input[:, :-1] will have length max_spa_len - 1,\n",
        "    # but the positional encoding is applied to the original embedded_tgt which has length max_spa_len.\n",
        "    MAX_SEQ_LEN = max(max_eng_len, max_spa_len)\n",
        "    print(f\"Determined MAX_SEQ_LEN from data: {MAX_SEQ_LEN}\")\n",
        "\n",
        "\n",
        "    # Define special token strings\n",
        "    PAD_TOKEN = '<pad>'\n",
        "    SOS_TOKEN = '<sos>'\n",
        "    EOS_TOKEN = '<eos>'\n",
        "\n",
        "    # --- 1. Model Hyperparameters ---\n",
        "    D_MODEL = 256 # Reduced for faster training (Original paper: 512)\n",
        "    N_HEADS = 8\n",
        "    N_ENCODER_LAYERS = 3 # Reduced (Original paper: 6)\n",
        "    N_DECODER_LAYERS = 3 # Reduced (Original paper: 6)\n",
        "    D_FF = 512      # Reduced (Original paper: 2048)\n",
        "    DROPOUT = 0.1\n",
        "\n",
        "    # --- 2. Training Parameters ---\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 10\n",
        "    LEARNING_RATE = 0.0005 # Adjusted common starting LR\n",
        "    WARMUP_STEPS = 0 # For simplified LR scheduler. For Transformer paper schedule, use e.g. 4000\n",
        "    MODEL_SAVE_PATH = 'best_transformer_model.pt'\n",
        "\n",
        "    # --- 3. Device Configuration ---\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # --- 4. Initialize Model ---\n",
        "    model = Transformer(\n",
        "        # Pass the integer size of the vocabulary, not the array itself\n",
        "        src_vocab_size=SRC_VOCAB_SIZE,\n",
        "        tgt_vocab_size=TGT_VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_encoder_layers=N_ENCODER_LAYERS,\n",
        "        n_decoder_layers=N_DECODER_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        max_len=MAX_SEQ_LEN, # Use the determined MAX_SEQ_LEN\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "\n",
        "    print(f'Transformer Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.')\n",
        "\n",
        "    # --- 5. (Optional) Check a batch shape after loading dataloader ---\n",
        "    # This confirms the actual tensor shapes being fed into the model\n",
        "    # for src_batch, tgt_batch in train_dataloader:\n",
        "    #     print(f\"Example batch shapes from train_dataloader: src={src_batch.shape}, tgt={tgt_batch.shape}\")\n",
        "    #     break # Only check the first batch\n",
        "\n",
        "\n",
        "    # --- 6. Train the Model ---\n",
        "    trained_model = train_transformer_model(\n",
        "        model, train_dataloader, test_dataloader, NUM_EPOCHS, device,\n",
        "        LEARNING_RATE, WARMUP_STEPS,\n",
        "        src_pad_idx=eng_tok_stoi['<pad>'],  # Pass English pad index\n",
        "        tgt_pad_idx=spa_tok_stoi['<pad>'],  # Pass Spanish pad index (for loss and masks)\n",
        "        model_save_path=MODEL_SAVE_PATH\n",
        "    )\n",
        "\n",
        "\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
        "    print(f\"\\nLoaded best model from {MODEL_SAVE_PATH} for translation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2emlAfTe2LNr"
      },
      "source": [
        "# TRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L92o0VTsyxiR",
        "outputId": "31c5c039-1e02-4b0c-ffe3-bcd71b4a29b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Translation Examples ---\n",
            "English: \"hello world\"\n",
            "Spanish: \"dile hola el mundo\"\n",
            "--------------------\n",
            "English: \"how are you\"\n",
            "Spanish: \"¿ cómo te estás tú !\"\n",
            "--------------------\n",
            "English: \"i am learning spanish\"\n",
            "Spanish: \"estoy aprendiendo español\"\n",
            "--------------------\n",
            "English: \"thank you very much\"\n",
            "Spanish: \"gracias mucho mucho\"\n",
            "--------------------\n",
            "English: \"this is a test sentence\"\n",
            "Spanish: \"esta frase es una frase\"\n",
            "--------------------\n",
            "English: \"i like apples and oranges\"\n",
            "Spanish: \"me gustan las manzanas y las naranjas\"\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def translate_sentence(sentence: str, model: Transformer, spacy_en, spacy_es,\n",
        "                       eng_stoi: dict, spa_itos: dict, englen: int, spalen: int,\n",
        "                       device: torch.device, src_pad_idx: int, sos_idx: int, eos_idx: int):\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # 1. Preprocess the input sentence\n",
        "    # Tokenize the English sentence\n",
        "    tokens = [tok.text.lower() for tok in spacy_en.tokenizer(sentence)]\n",
        "    # Ensure sentence is not empty after tokenization\n",
        "    if not tokens:\n",
        "        return \"Error: Input sentence is empty or could not be tokenized.\"\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    # Handle unknown tokens (tokens not in eng_stoi) - replace with a special <unk> token if available,\n",
        "    # or handle gracefully. Assuming no <unk> for simplicity, skip unknown tokens or use a default.\n",
        "    # A better approach would include an <unk> token in the vocabulary.\n",
        "    # For this example, let's just skip unknown tokens, which might lead to errors if a token is critical.\n",
        "    # Let's add a check and use a placeholder or skip. A more robust solution needs an <unk> token.\n",
        "    src_indices = [eng_stoi.get(token, src_pad_idx) for token in tokens] # Using PAD_TOKEN for simplicity if unknown\n",
        "\n",
        "    # Add padding to match the max length used during training\n",
        "    if len(src_indices) < englen:\n",
        "        src_indices += [src_pad_idx] * (englen - len(src_indices))\n",
        "    else:\n",
        "        # Truncate if longer than max length used in training (less common in translation inference)\n",
        "        src_indices = src_indices[:englen]\n",
        "        print(f\"Warning: Input sentence truncated to {englen} tokens.\")\n",
        "\n",
        "\n",
        "    # Convert to tensor and add batch dimension (batch_size=1)\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device) # [1, englen]\n",
        "\n",
        "    # 2. Encode the source sequence\n",
        "    with torch.no_grad():\n",
        "        # Create source padding mask for the encoder\n",
        "        src_mask = model.create_padding_mask(src_tensor, src_pad_idx) # [1, 1, 1, englen]\n",
        "        encoder_output = model.encoder(src_tensor, src_mask) # [1, englen, D_MODEL]\n",
        "\n",
        "    # 3. Decode the target sequence word by word (autonomously)\n",
        "    # Start with the <sos> token\n",
        "    # Initially, the target sequence for the decoder is just the <sos> token\n",
        "    tgt_indices = [sos_idx]\n",
        "    # Convert to tensor and add batch dimension\n",
        "    tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device) # [1, 1]\n",
        "\n",
        "    # Iterate until <eos> is predicted or maximum sequence length is reached\n",
        "    for _ in range(spalen - 1): # spalen is the max length including <sos> and <eos>\n",
        "        with torch.no_grad():\n",
        "            # Create combined target mask for the current tgt_tensor\n",
        "            # Need padding mask for the current tgt_tensor (only <sos> initially, then grows)\n",
        "            # Need look-ahead mask for the current length of tgt_tensor\n",
        "            tgt_padding_mask = model.create_padding_mask(tgt_tensor, src_pad_idx) # Use src_pad_idx if pad_idx is shared [1, 1, 1, current_tgt_len]\n",
        "                                                                               # Or use the correct target pad idx if different. Assuming shared for <pad> token.\n",
        "            look_ahead_mask = model.create_look_ahead_mask(tgt_tensor.size(1), device) # [1, 1, current_tgt_len, current_tgt_len]\n",
        "            combined_tgt_mask = tgt_padding_mask & look_ahead_mask\n",
        "\n",
        "            # Forward pass through the decoder\n",
        "            # encoder_output is reused at each step\n",
        "            # src_mask is reused at each step\n",
        "            # tgt_tensor grows by one token at each step\n",
        "            output_logits = model.decoder(tgt_tensor, encoder_output, src_mask, combined_tgt_mask)\n",
        "            # output_logits shape: [1, current_tgt_len, TGT_VOCAB_SIZE]\n",
        "\n",
        "            # Get the prediction for the *last* token in the sequence\n",
        "            prediction = output_logits[:, -1, :] # [1, TGT_VOCAB_SIZE]\n",
        "            predicted_token_index = prediction.argmax(dim=-1).item() # Get the index of the most probable token\n",
        "\n",
        "        # Append the predicted token index to the target sequence\n",
        "        tgt_indices.append(predicted_token_index)\n",
        "\n",
        "        # Convert the updated target indices to a tensor for the next step\n",
        "        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device) # [1, current_tgt_len + 1]\n",
        "\n",
        "        # If the predicted token is <eos>, stop decoding\n",
        "        if predicted_token_index == eos_idx:\n",
        "            break\n",
        "\n",
        "        # If the target sequence reaches the max length, stop decoding\n",
        "        if len(tgt_indices) >= spalen:\n",
        "            print(f\"Warning: Decoding stopped at max length {spalen}.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    # 4. Convert predicted indices back to tokens\n",
        "    # Exclude the <sos> token from the output translation\n",
        "    translated_tokens = [spa_itos.get(idx, '<unk>') for idx in tgt_indices[1:]] # Use <unk> if index not found\n",
        "\n",
        "    # Remove <eos> token if it was the last predicted token\n",
        "    if translated_tokens and translated_tokens[-1] == EOS_TOKEN:\n",
        "        translated_tokens = translated_tokens[:-1]\n",
        "\n",
        "    # Join tokens to form the translated sentence\n",
        "    # Decide how to join (e.g., with space). For Spanish, usually space is appropriate.\n",
        "    translated_sentence = \" \".join(translated_tokens)\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "# --- Example Usage (assuming the rest of your code has run successfully) ---\n",
        "\n",
        "# Ensure the model is on the correct device after loading\n",
        "model.to(device)\n",
        "\n",
        "# Assuming these dictionaries and lengths are available from previous steps\n",
        "# eng_tok_stoi, spa_tok_itos, englen, spalen, device\n",
        "# You also need the indices for special tokens\n",
        "try:\n",
        "    src_pad_idx_val = eng_tok_stoi[PAD_TOKEN] # Assuming same pad token index in src and tgt\n",
        "    sos_idx_val = spa_tok_stoi[SOS_TOKEN]\n",
        "    eos_idx_val = spa_tok_stoi[EOS_TOKEN]\n",
        "    # If <pad> has a different index in Spanish vocab, use that for tgt_mask in inference:\n",
        "    # tgt_pad_idx_val = spa_tok_stoi[PAD_TOKEN]\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Special token index not found in vocabulary: {e}\")\n",
        "    print(\"Please ensure '<pad>', '<sos>', and '<eos>' are in your vocabularies.\")\n",
        "    # Handle error, e.g., exit or use default values if applicable\n",
        "    exit() # Exit if required indices are missing\n",
        "\n",
        "\n",
        "# Example sentences to translate\n",
        "sentences_to_translate = [\n",
        "    \"hello world\",\n",
        "    \"how are you\",\n",
        "    \"i am learning spanish\",\n",
        "    \"thank you very much\",\n",
        "    \"this is a test sentence\",\n",
        "    \"i like apples and oranges\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Translation Examples ---\")\n",
        "for sentence in sentences_to_translate:\n",
        "    translated = translate_sentence(\n",
        "        sentence=sentence,\n",
        "        model=model,\n",
        "        spacy_en=spacy_en, # Assuming spacy_en is loaded\n",
        "        spacy_es=spacy_es, # Assuming spacy_es is loaded\n",
        "        eng_stoi=eng_tok_stoi,\n",
        "        spa_itos=spa_tok_itos,\n",
        "        englen=englen,\n",
        "        spalen=spalen, # Use the length calculated *after* adding sos/eos/pad\n",
        "        device=device,\n",
        "        src_pad_idx=src_pad_idx_val, # Use the English pad index\n",
        "        sos_idx=sos_idx_val,\n",
        "        eos_idx=eos_idx_val\n",
        "    )\n",
        "    print(f\"English: \\\"{sentence}\\\"\")\n",
        "    print(f\"Spanish: \\\"{translated}\\\"\")\n",
        "    print(\"-\" * 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_nHlhLH2GU6"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvHt-hzg09N3",
        "outputId": "349a2ac6-1eeb-4f1f-94b7-8096fcf5ddff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Translating: \"i love spanish\" ---\n",
            "English: \"i love spanish\"\n",
            "Spanish: \"me encanta español\"\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.to(device)\n",
        "try:\n",
        "  # Use the actual padding index from the English vocabulary\n",
        "  src_pad_idx_val = eng_tok_stoi['<pad>']\n",
        "  sos_idx_val = spa_tok_stoi['<sos>']\n",
        "  eos_idx_val = spa_tok_stoi['<eos>']\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Special token index not found in vocabulary: {e}\")\n",
        "    print(\"Please ensure '<pad>', '<sos>', and '<eos>' are in your vocabularies.\")\n",
        "    # Handle error, e.g., exit or use default values if applicable\n",
        "    exit() # Exit if required indices are missing\n",
        "\n",
        "\n",
        "# Sentence to translate\n",
        "sentence_to_translate = 'i love spanish'\n",
        "\n",
        "print(f\"\\n--- Translating: \\\"{sentence_to_translate}\\\" ---\")\n",
        "\n",
        "translated = translate_sentence(\n",
        "    sentence=sentence_to_translate,\n",
        "    model=model,\n",
        "    spacy_en=spacy_en, # Assuming spacy_en is loaded\n",
        "    spacy_es=spacy_es, # Assuming spacy_es is loaded\n",
        "    eng_stoi=eng_tok_stoi,\n",
        "    spa_itos=spa_tok_itos,\n",
        "    englen=englen,\n",
        "    spalen=spalen,\n",
        "    device=device,\n",
        "    src_pad_idx=src_pad_idx_val, # Use the English pad index\n",
        "    sos_idx=sos_idx_val,\n",
        "    eos_idx=eos_idx_val\n",
        ")\n",
        "\n",
        "print(f\"English: \\\"{sentence_to_translate}\\\"\")\n",
        "print(f\"Spanish: \\\"{translated}\\\"\")\n",
        "print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJbAAyZb2d7I"
      },
      "source": [
        "**Here we can see the translation of the model is wayyy better from the translation from encoder deocoder architecture under same epoch . It's almost close to real translation with just 10 epoch**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
